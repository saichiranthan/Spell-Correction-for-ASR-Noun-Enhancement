{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce3e242",
   "metadata": {},
   "source": [
    "# Baseline models for Spell Correction\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "Lets set up our environment by importing the necessary libraries, then load the training and validation sets that was created and saved in the previous notebook.Using the ast library to safely convert the string representation of the target_nouns list back into an actual list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bcfe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Training data loaded with 7000 rows.\n",
      "Validation data loaded with 1500 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct sentences</th>\n",
       "      <th>ASR-generated incorrect transcriptions</th>\n",
       "      <th>correct_word_count</th>\n",
       "      <th>incorrect_word_count</th>\n",
       "      <th>error_pairs</th>\n",
       "      <th>error_pairs_diff</th>\n",
       "      <th>correct_cleaned_baseline</th>\n",
       "      <th>incorrect_cleaned_baseline</th>\n",
       "      <th>correct_cleaned_advanced</th>\n",
       "      <th>incorrect_cleaned_advanced</th>\n",
       "      <th>target_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patients are recommended to consult their heal...</td>\n",
       "      <td>Patients are recommended to consult their heal...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[('tizarex', 'tzx')]</td>\n",
       "      <td>[('tizarex', 'tzx')]</td>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "      <td>[patients, healthcare, provider, tizarex, guid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMYDIO FORTE is a powerful medication often us...</td>\n",
       "      <td>Imidio Forte is a powerful medication, often u...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[('amydio', 'imidio')]</td>\n",
       "      <td>[('amydio', 'imidio')]</td>\n",
       "      <td>amydio forte is a powerful medication often us...</td>\n",
       "      <td>imidio forte is a powerful medication often us...</td>\n",
       "      <td>amydio forte is a powerful medication often us...</td>\n",
       "      <td>imidio forte is a powerful medication often us...</td>\n",
       "      <td>[amydio, forte, medication, pain, inflammation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provera 40 contains medroxyprogesterone acetat...</td>\n",
       "      <td>Provera-40 contains medrexiprogesterone acetat...</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>[('provera', 'provera40'), ('40', 'contains'),...</td>\n",
       "      <td>[('provera', 'provera-40'), ('medroxyprogester...</td>\n",
       "      <td>provera 40 contains medroxyprogesterone acetat...</td>\n",
       "      <td>provera-40 contains medrexiprogesterone acetat...</td>\n",
       "      <td>provera 40 contains medroxyprogesterone acetat...</td>\n",
       "      <td>provera-40 contains medrexiprogesterone acetat...</td>\n",
       "      <td>[provera, medroxyprogesterone, acetate, hormon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CEEMI-O is a popular over-the-counter medicati...</td>\n",
       "      <td>Simi, oh, is a popular over-the-counter medica...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>[('ceemio', 'simi'), ('is', 'oh'), ('a', 'is')...</td>\n",
       "      <td>[('ceemi-o', 'simi')]</td>\n",
       "      <td>ceemi-o is a popular over-the-counter medicati...</td>\n",
       "      <td>simi oh is a popular over-the-counter medicati...</td>\n",
       "      <td>ceemi-o is a popular over-the-counter medicati...</td>\n",
       "      <td>simi oh is a popular over-the-counter medicati...</td>\n",
       "      <td>[ceemi, o, counter, medication, flu, symptoms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L-arginine SR is a popular supplement known fo...</td>\n",
       "      <td>LRG9-SR is a popular supplement known for its ...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>[('larginine', 'lrg9sr'), ('sr', 'is'), ('is',...</td>\n",
       "      <td>[('l-arginine', 'lrg9-sr')]</td>\n",
       "      <td>l-arginine sr is a popular supplement known fo...</td>\n",
       "      <td>lrg9-sr is a popular supplement known for its ...</td>\n",
       "      <td>l-arginine sr is a popular supplement known fo...</td>\n",
       "      <td>lrg9-sr is a popular supplement known for its ...</td>\n",
       "      <td>[l, arginine, sr, supplement, benefits, health]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   correct sentences  \\\n",
       "0  Patients are recommended to consult their heal...   \n",
       "1  AMYDIO FORTE is a powerful medication often us...   \n",
       "2  Provera 40 contains medroxyprogesterone acetat...   \n",
       "3  CEEMI-O is a popular over-the-counter medicati...   \n",
       "4  L-arginine SR is a popular supplement known fo...   \n",
       "\n",
       "              ASR-generated incorrect transcriptions  correct_word_count  \\\n",
       "0  Patients are recommended to consult their heal...                  14   \n",
       "1  Imidio Forte is a powerful medication, often u...                  14   \n",
       "2  Provera-40 contains medrexiprogesterone acetat...                  13   \n",
       "3  Simi, oh, is a popular over-the-counter medica...                  12   \n",
       "4  LRG9-SR is a popular supplement known for its ...                  14   \n",
       "\n",
       "   incorrect_word_count                                        error_pairs  \\\n",
       "0                    14                               [('tizarex', 'tzx')]   \n",
       "1                    14                             [('amydio', 'imidio')]   \n",
       "2                    12  [('provera', 'provera40'), ('40', 'contains'),...   \n",
       "3                    13  [('ceemio', 'simi'), ('is', 'oh'), ('a', 'is')...   \n",
       "4                    13  [('larginine', 'lrg9sr'), ('sr', 'is'), ('is',...   \n",
       "\n",
       "                                    error_pairs_diff  \\\n",
       "0                               [('tizarex', 'tzx')]   \n",
       "1                             [('amydio', 'imidio')]   \n",
       "2  [('provera', 'provera-40'), ('medroxyprogester...   \n",
       "3                              [('ceemi-o', 'simi')]   \n",
       "4                        [('l-arginine', 'lrg9-sr')]   \n",
       "\n",
       "                            correct_cleaned_baseline  \\\n",
       "0  patients are recommended to consult their heal...   \n",
       "1  amydio forte is a powerful medication often us...   \n",
       "2  provera 40 contains medroxyprogesterone acetat...   \n",
       "3  ceemi-o is a popular over-the-counter medicati...   \n",
       "4  l-arginine sr is a popular supplement known fo...   \n",
       "\n",
       "                          incorrect_cleaned_baseline  \\\n",
       "0  patients are recommended to consult their heal...   \n",
       "1  imidio forte is a powerful medication often us...   \n",
       "2  provera-40 contains medrexiprogesterone acetat...   \n",
       "3  simi oh is a popular over-the-counter medicati...   \n",
       "4  lrg9-sr is a popular supplement known for its ...   \n",
       "\n",
       "                            correct_cleaned_advanced  \\\n",
       "0  patients are recommended to consult their heal...   \n",
       "1  amydio forte is a powerful medication often us...   \n",
       "2  provera 40 contains medroxyprogesterone acetat...   \n",
       "3  ceemi-o is a popular over-the-counter medicati...   \n",
       "4  l-arginine sr is a popular supplement known fo...   \n",
       "\n",
       "                          incorrect_cleaned_advanced  \\\n",
       "0  patients are recommended to consult their heal...   \n",
       "1  imidio forte is a powerful medication often us...   \n",
       "2  provera-40 contains medrexiprogesterone acetat...   \n",
       "3  simi oh is a popular over-the-counter medicati...   \n",
       "4  lrg9-sr is a popular supplement known for its ...   \n",
       "\n",
       "                                        target_nouns  \n",
       "0  [patients, healthcare, provider, tizarex, guid...  \n",
       "1    [amydio, forte, medication, pain, inflammation]  \n",
       "2  [provera, medroxyprogesterone, acetate, hormon...  \n",
       "3     [ceemi, o, counter, medication, flu, symptoms]  \n",
       "4    [l, arginine, sr, supplement, benefits, health]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from tqdm.notebook import tqdm\n",
    "import ast # To safely evaluate string representations of lists\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import spacy\n",
    "\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the spaCy model for noun extraction during evaluation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# Load the preprocessed datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('../dataset/train.csv')\n",
    "    val_df = pd.read_csv('../dataset/validation.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'train.csv' and 'validation.csv' are in the 'dataset' directory.\")\n",
    "    # Create empty dataframes to avoid further errors if files are not found\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "\n",
    "# Safely convert the 'target_nouns' column from string back to a list\n",
    "# This is necessary because CSVs don't store list types natively\n",
    "if 'target_nouns' in val_df.columns:\n",
    "    val_df['target_nouns'] = val_df['target_nouns'].apply(ast.literal_eval)\n",
    "\n",
    "print(f\"Training data loaded with {len(train_df)} rows.\")\n",
    "print(f\"Validation data loaded with {len(val_df)} rows.\")\n",
    "\n",
    "display(val_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ba5f3",
   "metadata": {},
   "source": [
    "## 2. Baseline Model: Dictionary & Levenshtein Distance\n",
    "\n",
    "Our baseline model will follow a traditional spell-correction approach as outlined in the assignment:\n",
    "\n",
    "1.  **Build a Dictionary:** We will create a comprehensive vocabulary set from all the words in the `correct_cleaned_baseline` column of our **training data**. This set will serve as our ground truth for \"correct\" words.\n",
    "2.  **Correction Logic:** For each word in an incorrect sentence, we will check if it exists in our dictionary.\n",
    "    *   If it does, we assume it's correct.\n",
    "    *   If it doesn't, we search our dictionary for the word with the smallest **Levenshtein distance** (edit distance).\n",
    "    *   If this smallest distance is below a set threshold (e.g., 2), we replace the incorrect word. Otherwise, we leave it unchanged to avoid incorrect \"corrections.\"\n",
    "\n",
    "#### 2.1 Build the Vocabulary Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79023155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built with 9188 unique words.\n",
      "Sample words from vocabulary: ['highlight', 'clohart-a', 'oftec', 'sapsyl-fe', 'desthama', 'caspofungin', 'greatly', 'flagel', 'cenegermin-bkbj', 'breakthrough']\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary from the correct sentences in the training data\n",
    "# Using a set provides O(1) average time complexity for lookups, which is very fast.\n",
    "if not train_df.empty:\n",
    "    all_correct_words = ' '.join(train_df['correct_cleaned_baseline'].astype(str)).split()\n",
    "    vocabulary = set(all_correct_words)\n",
    "    print(f\"Vocabulary built with {len(vocabulary)} unique words.\")\n",
    "else:\n",
    "    vocabulary = set()\n",
    "    print(\"Vocabulary is empty as training data could not be loaded.\")\n",
    "\n",
    "# Let's look at a few words from our vocabulary\n",
    "print(\"Sample words from vocabulary:\", list(vocabulary)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806f487",
   "metadata": {},
   "source": [
    "#### 2.2 Implement the Correction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91f2baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  'take one tablet of lisinopril for high blod presure'\n",
      "Corrected: 'take one tablet of fosinopril for high blood pressure'\n"
     ]
    }
   ],
   "source": [
    "# This dictionary will act as a cache to store corrections we've already computed,\n",
    "# speeding up the process significantly if the same incorrect word appears many times.\n",
    "correction_cache = {}\n",
    "MAX_EDIT_DISTANCE = 2 # The maximum distance to consider a word for correction\n",
    "\n",
    "def get_correction(word, vocab):\n",
    "    \"\"\"\n",
    "    Finds the best correction for a word from the vocabulary based on Levenshtein distance.\n",
    "    \"\"\"\n",
    "    # If the word is already in our cache, return the cached correction\n",
    "    if word in correction_cache:\n",
    "        return correction_cache[word]\n",
    "    \n",
    "    # If the word is in our vocabulary, it's correct\n",
    "    if word in vocab:\n",
    "        correction_cache[word] = word\n",
    "        return word\n",
    "\n",
    "    # Find the best suggestion from the vocabulary\n",
    "    suggestions = [(edit_distance(word, v_word), v_word) for v_word in vocab if abs(len(word) - len(v_word)) <= MAX_EDIT_DISTANCE]\n",
    "    \n",
    "    if not suggestions:\n",
    "        correction_cache[word] = word # No suggestions found\n",
    "        return word\n",
    "\n",
    "    # Get the suggestion with the minimum edit distance\n",
    "    best_suggestion = min(suggestions, key=lambda x: x[0])\n",
    "    \n",
    "    # If the best suggestion is within our threshold, use it\n",
    "    if best_suggestion[0] <= MAX_EDIT_DISTANCE:\n",
    "        correction_cache[word] = best_suggestion[1]\n",
    "        return best_suggestion[1]\n",
    "    else:\n",
    "        # The closest word is still too different, so assume the original word is correct\n",
    "        correction_cache[word] = word\n",
    "        return word\n",
    "\n",
    "def baseline_sentence_correction(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Applies the correction logic to an entire sentence.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    corrected_words = [get_correction(word, vocab) for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# --- Test the function on a sample sentence ---\n",
    "sample_incorrect_sentence = \"take one tablet of lisinopril for high blod presure\"\n",
    "corrected_sentence = baseline_sentence_correction(sample_incorrect_sentence, vocabulary)\n",
    "print(f\"Original:  '{sample_incorrect_sentence}'\")\n",
    "print(f\"Corrected: '{corrected_sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ecd5b",
   "metadata": {},
   "source": [
    "#### 2.3 Apply Correction to the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761c4c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying baseline correction to the validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e16bd1ff4342088171e69cd001b1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect_cleaned_baseline</th>\n",
       "      <th>baseline_predicted</th>\n",
       "      <th>correct_cleaned_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "      <td>patients are recommended to consult their heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imidio forte is a powerful medication often us...</td>\n",
       "      <td>imidio forte is a powerful medication often us...</td>\n",
       "      <td>amydio forte is a powerful medication often us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>provera-40 contains medrexiprogesterone acetat...</td>\n",
       "      <td>provera-d contains medrexiprogesterone acetate...</td>\n",
       "      <td>provera 40 contains medroxyprogesterone acetat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simi oh is a popular over-the-counter medicati...</td>\n",
       "      <td>some h is a popular over-the-counter medicatio...</td>\n",
       "      <td>ceemi-o is a popular over-the-counter medicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lrg9-sr is a popular supplement known for its ...</td>\n",
       "      <td>lrg9-sr is a popular supplement known for its ...</td>\n",
       "      <td>l-arginine sr is a popular supplement known fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          incorrect_cleaned_baseline  \\\n",
       "0  patients are recommended to consult their heal...   \n",
       "1  imidio forte is a powerful medication often us...   \n",
       "2  provera-40 contains medrexiprogesterone acetat...   \n",
       "3  simi oh is a popular over-the-counter medicati...   \n",
       "4  lrg9-sr is a popular supplement known for its ...   \n",
       "\n",
       "                                  baseline_predicted  \\\n",
       "0  patients are recommended to consult their heal...   \n",
       "1  imidio forte is a powerful medication often us...   \n",
       "2  provera-d contains medrexiprogesterone acetate...   \n",
       "3  some h is a popular over-the-counter medicatio...   \n",
       "4  lrg9-sr is a popular supplement known for its ...   \n",
       "\n",
       "                            correct_cleaned_baseline  \n",
       "0  patients are recommended to consult their heal...  \n",
       "1  amydio forte is a powerful medication often us...  \n",
       "2  provera 40 contains medroxyprogesterone acetat...  \n",
       "3  ceemi-o is a popular over-the-counter medicati...  \n",
       "4  l-arginine sr is a popular supplement known fo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not val_df.empty:\n",
    "    print(\"Applying baseline correction to the validation set...\")\n",
    "    val_df['baseline_predicted'] = val_df['incorrect_cleaned_baseline'].progress_apply(\n",
    "        lambda x: baseline_sentence_correction(x, vocabulary)\n",
    "    )\n",
    "    print(\"Correction complete.\")\n",
    "    display(val_df[['incorrect_cleaned_baseline', 'baseline_predicted', 'correct_cleaned_baseline']].head())\n",
    "else:\n",
    "    print(\"Validation DataFrame is empty. Skipping correction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5f4db",
   "metadata": {},
   "source": [
    "## **3. Evaluation**\n",
    "\n",
    "Evaluating the performance of our baseline model using the metrics specified in the assignment.\n",
    "\n",
    "#### **3.1 Word-Level Accuracy**\n",
    "\n",
    "This metric calculates the percentage of words that were correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9086b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level Accuracy: 68.60%\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_accuracy(df, true_col, pred_col):\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    for index, row in df.iterrows():\n",
    "        true_words = str(row[true_col]).split()\n",
    "        pred_words = str(row[pred_col]).split()\n",
    "        for i in range(min(len(true_words), len(pred_words))):\n",
    "            if true_words[i] == pred_words[i]:\n",
    "                correct_words += 1\n",
    "        total_words += len(true_words)\n",
    "    return (correct_words / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "if 'baseline_predicted' in val_df.columns:\n",
    "    word_acc = calculate_word_accuracy(val_df, 'correct_cleaned_baseline', 'baseline_predicted')\n",
    "    print(f\"Word-Level Accuracy: {word_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d958b72",
   "metadata": {},
   "source": [
    "#### **3.2 Noun-Specific Accuracy**\n",
    "\n",
    "This is a crucial metric for this assignment. We'll measure how many of the required medical nouns were correctly identified in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4393a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun-Specific Accuracy: 73.89%\n"
     ]
    }
   ],
   "source": [
    "def calculate_noun_accuracy(df, true_nouns_col, pred_sent_col):\n",
    "    total_true_nouns = 0\n",
    "    correctly_predicted_nouns = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        true_nouns = set(row[true_nouns_col])\n",
    "        \n",
    "        # Extract nouns from the predicted sentence\n",
    "        pred_doc = nlp(str(row[pred_sent_col]))\n",
    "        pred_nouns = set([token.text for token in pred_doc if token.pos_ in ('NOUN', 'PROPN')])\n",
    "        \n",
    "        # Find the intersection of the two sets\n",
    "        correctly_predicted_nouns += len(true_nouns.intersection(pred_nouns))\n",
    "        total_true_nouns += len(true_nouns)\n",
    "        \n",
    "    return (correctly_predicted_nouns / total_true_nouns) * 100 if total_true_nouns > 0 else 0\n",
    "\n",
    "if 'baseline_predicted' in val_df.columns:\n",
    "    noun_acc = calculate_noun_accuracy(val_df, 'target_nouns', 'baseline_predicted')\n",
    "    print(f\"Noun-Specific Accuracy: {noun_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a31487",
   "metadata": {},
   "source": [
    "#### **3.3 BLEU Score**\n",
    "\n",
    "BLEU score measures how similar the predicted sentence is to the reference sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b46a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 79.08\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
    "if 'baseline_predicted' in val_df.columns:\n",
    "    bleu_scores = []\n",
    "    # Instantiate the smoothing function once before the loop\n",
    "    chencherry = SmoothingFunction()\n",
    "    \n",
    "    for index, row in val_df.iterrows():\n",
    "        reference = [str(row['correct_cleaned_baseline']).split()]\n",
    "        candidate = str(row['baseline_predicted']).split()\n",
    "        \n",
    "        # The candidate sentence must not be empty\n",
    "        if not candidate:\n",
    "            bleu_scores.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Use the instantiated smoothing function\n",
    "        # We use method1 as a simple, standard smoothing technique\n",
    "        score = sentence_bleu(reference, candidate, smoothing_function=chencherry.method1)\n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    avg_bleu = np.mean(bleu_scores) * 100 # Often expressed as a percentage\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.2f}\")\n",
    "else:\n",
    "    print(\"Column 'baseline_predicted' not found. Skipping BLEU score calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef436ec1",
   "metadata": {},
   "source": [
    "### **4. Analysis and Conclusion**\n",
    "\n",
    "Summarizing the findings and analyzing the baseline model's performance.\n",
    "\n",
    "**Performance Summary:**\n",
    "\n",
    "| Metric | Score |\n",
    "| :--- | :--- |\n",
    "| Word-Level Accuracy | 68.60 % |\n",
    "| Noun-Specific Accuracy | 73.89% |\n",
    "| Average BLEU Score | 79.08 |\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "The baseline model provides a solid starting point. It is effective at correcting **simple, character-level misspellings** for words that are present in our training vocabulary (e.g., `blod` -> `blood`).\n",
    "\n",
    "However, it has several significant weaknesses that were predicted during our EDA:\n",
    "\n",
    "*   **Segmentation Errors:** The model is completely unable to handle segmentation errors. It cannot merge `\"health care\"` into `\"healthcare\"` because it processes words one by one.\n",
    "*   **Out-of-Vocabulary (OOV) Words:** If a misspelled word's correct form was not in our training data, the model cannot correct it.\n",
    "*   **Context-Blindness:** The model has no understanding of context. It might make a phonetically plausible but medically incorrect substitution if that word happens to be closer in edit distance.\n",
    "*   **Computational Cost:** The process of calculating edit distance against a large vocabulary for every incorrect word is computationally expensive and slow.\n",
    "\n",
    "**Conclusion:** This baseline fulfills the assignment requirements and clearly demonstrates the limitations of traditional, non-contextual spell-correction methods. It sets a clear performance benchmark that we will aim to surpass with our advanced transformer-based models in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1cf55",
   "metadata": {},
   "source": [
    "## **5. Discussion: Other Baseline Approaches (N-gram Model)**\n",
    "\n",
    "The assignment also lists N-gram language models as a traditional spell-correction approach. It's important to discuss how this method works and how it differs from our implemented baseline.\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "An N-gram language model uses statistical properties of the text to determine the probability of a sequence of words. For spell correction, it leverages this by choosing the correction candidate that forms the most probable sequence with its surrounding words (its context).\n",
    "\n",
    "**Example Workflow:**\n",
    "\n",
    "1.  **Input Sentence:** \"take one tablet of **licinopril** for high **blod** presure\"\n",
    "2.  **Candidate Generation:** For a misspelled word like `\"blod\"`, candidates are generated (e.g., \"blood\", \"blond\", \"bold\").\n",
    "3.  **Contextual Scoring:** The N-gram model evaluates each candidate based on its context. It would calculate:\n",
    "    *   `Probability(\"high blood pressure\")`\n",
    "    *   `Probability(\"high blond pressure\")`\n",
    "4.  **Selection:** Since \"high blood pressure\" is a very common trigram in medical and general text, it would receive a much higher probability score. The model would therefore select \"blood\" as the correct replacement.\n",
    "\n",
    "**Advantages over our current baseline:**\n",
    "\n",
    "*   **Context-Aware:** This is its primary advantage. It can resolve ambiguities where multiple words have a similar edit distance. For example, it could distinguish between correcting \"flor\" to \"floor\" or \"flour\" based on the preceding words (\"baking ___\" vs. \"on the ___\").\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "*   **Limited Context Window:** An N-gram model's context is limited to the `n-1` preceding words. It cannot capture long-range dependencies in a sentence.\n",
    "*   **Data Sparsity:** Many valid word sequences (especially for trigrams or higher) may not appear in the training data, leading to zero probabilities.\n",
    "*   **Inability to Handle Segmentation:** Like our current baseline, it still processes word-by-word and cannot effectively handle segmentation errors like merging `\"health care\"` into `\"healthcare\"`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
